{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7fe89e-2b39-4406-a1d8-940c7f2d1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary modules\n",
    "import os\n",
    "import sys\n",
    "import os, sys\n",
    "# from .../research/notebooks -> go up two levels to repo root\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as mtick\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score \n",
    "import pandas_datareader as pdr\n",
    "import math\n",
    "import datetime\n",
    "from datetime import datetime, timezone\n",
    "import itertools\n",
    "import ast\n",
    "import yfinance as yf\n",
    "import seaborn as sn\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from strategy_signal.trend_following_signal import (\n",
    "    apply_jupyter_fullscreen_css, get_trend_donchian_signal_for_portfolio_with_rolling_r_sqr_vol_of_vol\n",
    ")\n",
    "from portfolio.strategy_performance import (calculate_sharpe_ratio, calculate_calmar_ratio, calculate_CAGR, calculate_risk_and_performance_metrics,\n",
    "                                          calculate_compounded_cumulative_returns, estimate_fee_per_trade, rolling_sharpe_ratio)\n",
    "from utils import coinbase_utils as cn\n",
    "from portfolio import strategy_performance as perf\n",
    "from sizing import position_sizing_binary_utils as size_bin\n",
    "from sizing import position_sizing_continuous_utils as size_cont\n",
    "from strategy_signal import trend_following_signal as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225849fc-a842-4ca1-8c32-8a8322df1399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sizing.position_sizing_continuous_utils' from '/Users/adheerchauhan/Documents/git/trend_following/sizing/position_sizing_continuous_utils.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(cn)\n",
    "importlib.reload(perf)\n",
    "importlib.reload(tf)\n",
    "importlib.reload(size_bin)\n",
    "importlib.reload(size_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d241457-8851-4e83-907c-349ba262cecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root {\n",
       "    --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('Display.max_rows', None)\n",
    "pd.set_option('Display.max_columns',None)\n",
    "apply_jupyter_fullscreen_css()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d78759-406a-4ec7-8924-0513e80a34f7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7b3761-d3b8-45be-a460-ca06eca30796",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Config file for the strategy\n",
    "def load_prod_strategy_config(strategy_version='v0.1.0'):\n",
    "    nb_cwd = Path.cwd()  # git/trend_following/research/notebooks\n",
    "    config_path = (\n",
    "            nb_cwd.parents[1]  # -> git/trend_following\n",
    "            / \"live_strategy\"\n",
    "            / f\"trend_following_strategy_{strategy_version}-live\"\n",
    "            / \"config\"\n",
    "            / f\"trend_strategy_config_{strategy_version}.yaml\"\n",
    "    )\n",
    "\n",
    "    print(config_path)  # sanity check\n",
    "    print(config_path.exists())  # should be True\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "973ed336-d249-4b8e-b962-600604725165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal_performance(df_1, df_2, ticker):\n",
    "\n",
    "    fig = plt.figure(figsize=(20,12))\n",
    "    layout = (2,2)\n",
    "    signal_ax = plt.subplot2grid(layout, (0,0))\n",
    "    price_ax = signal_ax.twinx()\n",
    "    equity_curve_ax = plt.subplot2grid(layout, (0,1))\n",
    "    sharpe_ax = plt.subplot2grid(layout, (1,0))\n",
    "    portfolio_value_ax = plt.subplot2grid(layout, (1,1))\n",
    "\n",
    "    _ = signal_ax.plot(df_1.index, df_1[f'{ticker}_final_signal'], label='Orig Signal', alpha=0.9)\n",
    "    _ = signal_ax.plot(df_2.index, df_2[f'{ticker}_final_signal'], label='New Signal', alpha=0.9)\n",
    "    _ = price_ax.plot(df_1.index, df_2[f'{ticker}_open'], label='Price', alpha=0.7, linestyle='--', color='magenta')\n",
    "    _ = signal_ax.set_title(f'Orignal Signal vs New Signal')\n",
    "    _ = signal_ax.set_ylabel('Signal')\n",
    "    _ = signal_ax.set_xlabel('Date')\n",
    "    _ = signal_ax.legend(loc='upper left')\n",
    "    _ = signal_ax.grid()\n",
    "\n",
    "    _ = equity_curve_ax.plot(df_1.index, df_1[f'equity_curve'], label='Orig Signal', alpha=0.9)\n",
    "    _ = equity_curve_ax.plot(df_2.index, df_2[f'equity_curve'], label='New Signal', alpha=0.9)\n",
    "    _ = equity_curve_ax.set_title(f'Equity Curve')\n",
    "    _ = equity_curve_ax.set_ylabel('Equity Curve')\n",
    "    _ = equity_curve_ax.set_xlabel('Date')\n",
    "    _ = equity_curve_ax.legend(loc='upper left')\n",
    "    _ = equity_curve_ax.grid()\n",
    "\n",
    "    _ = sharpe_ax.plot(df_1.index, df_1[f'portfolio_rolling_sharpe_50'], label='Orig Signal', alpha=0.9)\n",
    "    _ = sharpe_ax.plot(df_2.index, df_2[f'portfolio_rolling_sharpe_50'], label='New Signal', alpha=0.9)\n",
    "    _ = sharpe_ax.set_title(f'Rolling Sharpe')\n",
    "    _ = sharpe_ax.set_ylabel(f'Rolling Sharpe')\n",
    "    _ = sharpe_ax.set_xlabel('Date')\n",
    "    _ = sharpe_ax.legend(loc='upper left')\n",
    "    _ = sharpe_ax.grid()\n",
    "\n",
    "    _ = portfolio_value_ax.plot(df_1.index, df_1[f'total_portfolio_value'], label='Orig Signal', alpha=0.9)\n",
    "    _ = portfolio_value_ax.plot(df_2.index, df_2[f'total_portfolio_value'], label='New Signal', alpha=0.9)\n",
    "    _ = portfolio_value_ax.set_title(f'Total Portfolio Value')\n",
    "    _ = portfolio_value_ax.set_ylabel('Portfolio Value')\n",
    "    _ = portfolio_value_ax.set_xlabel('Date')\n",
    "    _ = portfolio_value_ax.legend(loc='upper left')\n",
    "    _ = portfolio_value_ax.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d696f7-2242-43c5-94ac-3318bfa0694f",
   "metadata": {},
   "source": [
    "## Universe Expansion Trend Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aa4307c-37df-4d95-933c-5376ad40a5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adheerchauhan/Documents/git/trend_following/live_strategy/trend_following_strategy_v0.1.0-live/config/trend_strategy_config_v0.1.0.yaml\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cfg = load_prod_strategy_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a4972f0-0b3f-4234-b391-c027f4a02dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'portfolio': {'exchange': 'Coinbase Advanced', 'name': 'Trend Following'},\n",
       " 'run': {'start_date': '2022-04-01',\n",
       "  'end_date': '2025-07-31',\n",
       "  'use_specific_start_date': True,\n",
       "  'signal_start_date': '2022-04-01',\n",
       "  'warmup_days': 300,\n",
       "  'long_only': True,\n",
       "  'annual_trading_days': 365,\n",
       "  'initial_capital': 15000},\n",
       " 'universe': {'tickers': ['BTC-USD',\n",
       "   'ETH-USD',\n",
       "   'SOL-USD',\n",
       "   'ADA-USD',\n",
       "   'AVAX-USD']},\n",
       " 'data': {'use_coinbase_data': True,\n",
       "  'use_saved_files': True,\n",
       "  'saved_file_end_date': '2025-07-31',\n",
       "  'price_or_returns_calc': 'price',\n",
       "  'moving_avg_type': 'exponential'},\n",
       " 'signals': {'moving_average': {'fast_mavg': 20,\n",
       "   'slow_mavg': 200,\n",
       "   'mavg_stepsize': 8,\n",
       "   'mavg_z_score_window': 126},\n",
       "  'donchian': {'entry_rolling_donchian_window': 56,\n",
       "   'exit_rolling_donchian_window': 28,\n",
       "   'use_donchian_exit_gate': False},\n",
       "  'weighting': {'ma_crossover_signal_weight': 0.85,\n",
       "   'donchian_signal_weight': 0.15,\n",
       "   'weighted_signal_ewm_window': 4},\n",
       "  'activation': {'use_activation': False,\n",
       "   'tanh_activation_constant_dict': None},\n",
       "  'filters': {'rolling_r2': {'rolling_r2_window': 100,\n",
       "    'lower_r_sqr_limit': 0.45,\n",
       "    'upper_r_sqr_limit': 0.9,\n",
       "    'r2_smooth_window': 3,\n",
       "    'r2_confirm_days': 0,\n",
       "    'r2_strong_threshold': 0.75},\n",
       "   'vol_of_vol': {'log_std_window': 14,\n",
       "    'coef_of_variation_window': 20,\n",
       "    'vol_of_vol_z_score_window': 126,\n",
       "    'vol_of_vol_p_min': 0.1}}},\n",
       " 'risk_and_sizing': {'annualized_target_volatility': 0.45,\n",
       "  'volatility_window': 30,\n",
       "  'rolling_cov_window': 30,\n",
       "  'rolling_atr_window': 20,\n",
       "  'atr_multiplier': 1.75,\n",
       "  'stop_loss_strategy': 'Chandelier',\n",
       "  'highest_high_window': 56,\n",
       "  'rolling_sharpe_window': 50,\n",
       "  'cash_buffer_percentage': 0.15},\n",
       " 'execution_and_costs': {'transaction_cost_est': 0.001,\n",
       "  'passive_trade_rate': 0.05,\n",
       "  'notional_threshold_pct': 0.1,\n",
       "  'min_trade_notional_abs': 10,\n",
       "  'cooldown_counter_threshold': 7}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9801837-6ca3-4f25-8742-432e50c23e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prod Configuration (from cfg) ---\n",
    "\n",
    "# portfolio\n",
    "exchange = cfg['portfolio']['exchange']\n",
    "portfolio_name = cfg['portfolio']['name']\n",
    "\n",
    "start_date  = pd.Timestamp(cfg['run']['start_date']).date()\n",
    "end_date    = pd.Timestamp(cfg['run']['end_date']).date()\n",
    "use_specific_start_date = bool(cfg['run']['use_specific_start_date'])\n",
    "signal_start_date       = pd.Timestamp(cfg['run']['signal_start_date']).date()\n",
    "warmup_days = int(cfg['run']['warmup_days'])\n",
    "long_only = cfg['run']['long_only']\n",
    "annual_trading_days    = int(cfg['run']['annual_trading_days'])\n",
    "initial_capital        = float(cfg['run']['initial_capital'])\n",
    "\n",
    "# universe\n",
    "ticker_list = list(cfg['universe']['tickers'])\n",
    "\n",
    "# data\n",
    "use_coinbase_data      = bool(cfg['data']['use_coinbase_data'])\n",
    "use_saved_files        = bool(cfg['data']['use_saved_files'])\n",
    "saved_file_end_date    = str(cfg['data']['saved_file_end_date'])\n",
    "price_or_returns_calc    = str(cfg['data']['price_or_returns_calc'])\n",
    "moving_avg_type    = str(cfg['data']['moving_avg_type'])\n",
    "\n",
    "# signals.moving_average\n",
    "fast_mavg        = int(cfg['signals']['moving_average']['fast_mavg'])\n",
    "slow_mavg        = int(cfg['signals']['moving_average']['slow_mavg'])\n",
    "mavg_stepsize    = int(cfg['signals']['moving_average']['mavg_stepsize'])\n",
    "mavg_z_score_window = int(cfg['signals']['moving_average']['mavg_z_score_window'])\n",
    "\n",
    "# signals.donchian\n",
    "entry_rolling_donchian_window = int(cfg['signals']['donchian']['entry_rolling_donchian_window'])\n",
    "exit_rolling_donchian_window  = int(cfg['signals']['donchian']['exit_rolling_donchian_window'])\n",
    "use_donchian_exit_gate        = bool(cfg['signals']['donchian']['use_donchian_exit_gate'])\n",
    "\n",
    "# signals.weighting\n",
    "ma_crossover_signal_weight = float(cfg['signals']['weighting']['ma_crossover_signal_weight'])\n",
    "donchian_signal_weight     = float(cfg['signals']['weighting']['donchian_signal_weight'])\n",
    "weighted_signal_ewm_window = int(cfg['signals']['weighting']['weighted_signal_ewm_window'])  # (new config but same value)\n",
    "\n",
    "# signals.filters.rolling_r2\n",
    "rolling_r2_window   = int(cfg['signals']['filters']['rolling_r2']['rolling_r2_window'])\n",
    "lower_r_sqr_limit   = float(cfg['signals']['filters']['rolling_r2']['lower_r_sqr_limit'])\n",
    "upper_r_sqr_limit   = float(cfg['signals']['filters']['rolling_r2']['upper_r_sqr_limit'])\n",
    "r2_smooth_window    = int(cfg['signals']['filters']['rolling_r2']['r2_smooth_window'])\n",
    "r2_confirm_days     = int(cfg['signals']['filters']['rolling_r2']['r2_confirm_days'])\n",
    "r2_strong_threshold = float(cfg['signals']['filters']['rolling_r2']['r2_strong_threshold'])\n",
    "\n",
    "# signals.filters.vol_of_vol\n",
    "log_std_window            = int(cfg['signals']['filters']['vol_of_vol']['log_std_window'])\n",
    "coef_of_variation_window  = int(cfg['signals']['filters']['vol_of_vol']['coef_of_variation_window'])\n",
    "vol_of_vol_z_score_window = int(cfg['signals']['filters']['vol_of_vol']['vol_of_vol_z_score_window'])\n",
    "vol_of_vol_p_min          = float(cfg['signals']['filters']['vol_of_vol']['vol_of_vol_p_min'])\n",
    "\n",
    "# signals.activation\n",
    "use_activation              = bool(cfg['signals']['activation']['use_activation'])\n",
    "tanh_activation_constant_dict = cfg['signals']['activation']['tanh_activation_constant_dict']  # likely None\n",
    "\n",
    "# risk_and_sizing\n",
    "annualized_target_volatility = float(cfg['risk_and_sizing']['annualized_target_volatility'])\n",
    "volatility_window      = int(cfg['risk_and_sizing']['volatility_window'])\n",
    "rolling_cov_window     = int(cfg['risk_and_sizing']['rolling_cov_window'])\n",
    "rolling_atr_window     = int(cfg['risk_and_sizing']['rolling_atr_window'])\n",
    "atr_multiplier         = float(cfg['risk_and_sizing']['atr_multiplier'])\n",
    "stop_loss_strategy     = str(cfg['risk_and_sizing']['stop_loss_strategy'])\n",
    "highest_high_window    = int(cfg['risk_and_sizing']['highest_high_window'])\n",
    "rolling_sharpe_window    = int(cfg['risk_and_sizing']['rolling_sharpe_window'])\n",
    "cash_buffer_percentage = float(cfg['risk_and_sizing']['cash_buffer_percentage'])\n",
    "\n",
    "# execution_and_costs\n",
    "transaction_cost_est   = float(cfg['execution_and_costs']['transaction_cost_est'])\n",
    "passive_trade_rate     = float(cfg['execution_and_costs']['passive_trade_rate'])\n",
    "notional_threshold_pct = float(cfg['execution_and_costs']['notional_threshold_pct'])\n",
    "min_trade_notional_abs = float(cfg['execution_and_costs']['min_trade_notional_abs'])\n",
    "cooldown_counter_threshold = int(cfg['execution_and_costs']['cooldown_counter_threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "827c19aa-9c4d-4ea1-928c-37ac340eb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to get a list of all tradeable coins on Coinbase on any given day\n",
    "\n",
    "def product_to_dict(p):\n",
    "    # Pydantic v2\n",
    "    fn = getattr(p, \"model_dump\", None)\n",
    "    if callable(fn):\n",
    "        return fn(exclude_none=True)  # or fn() if you want Nones\n",
    "\n",
    "    # Pydantic v1\n",
    "    fn = getattr(p, \"dict\", None)\n",
    "    if callable(fn):\n",
    "        return fn()\n",
    "\n",
    "    # JSON fallbacks (v2 and v1 respectively)\n",
    "    fn = getattr(p, \"model_dump_json\", None)\n",
    "    if callable(fn):\n",
    "        return json.loads(p.model_dump_json())\n",
    "    fn = getattr(p, \"json\", None)\n",
    "    if callable(fn):\n",
    "        return json.loads(p.json())\n",
    "\n",
    "    # Last-resort: plain object\n",
    "    if hasattr(p, \"__dict__\"):\n",
    "        return {k: v for k, v in vars(p).items() if not k.startswith(\"_\")}\n",
    "\n",
    "    return {\"raw\": str(p)}\n",
    "\n",
    "## Get a snapshot of all the available coins to trade\n",
    "CANON_QUOTE = \"USD\"\n",
    "PRODUCTS_DIR = Path(\"/Users/adheerchauhan/Documents/git/trend_following/data_folder/universe/products\")\n",
    "PRODUCTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LIQUIDITY_DIR = Path(\"/Users/adheerchauhan/Documents/git/trend_following/data_folder/universe/liquidity\")\n",
    "LIQUIDITY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ELIGIBLE_DIR = Path(\"/Users/adheerchauhan/Documents/git/trend_following/data_folder/universe/eligible_products\")\n",
    "ELIGIBLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def coinbase_product_snapshot(client, asof=None, save=True):\n",
    "\n",
    "    asof = asof or datetime.now(timezone.utc).date().isoformat()\n",
    "    prod = client.get_products()['products']\n",
    "    rows = [product_to_dict(p) for p in prod]\n",
    "    df = pd.json_normalize(rows)\n",
    "\n",
    "    # optional: keep only columns you care about\n",
    "    reqd_cols = [\n",
    "        \"product_id\",\"base_currency_id\",\"quote_currency_id\",\"product_type\",\"status\",\n",
    "        \"trading_disabled\",\"is_disabled\",\"cancel_only\",\"limit_only\",\"post_only\",\"auction_mode\",\"view_only\",\n",
    "        \"base_increment\",\"quote_increment\",\"price_increment\",\"base_min_size\",\"quote_min_size\",\n",
    "        \"alias\",\"alias_to\",\"display_name\",\"product_venue\",\"new_at\",\"price\",\"approximate_quote_24h_volume\"\n",
    "    ]\n",
    "    df = df[reqd_cols]\n",
    "\n",
    "    # optional: coerce numerics\n",
    "    num_cols = [\"base_increment\",\"quote_increment\",\"price_increment\",\"base_min_size\",\"quote_min_size\",\n",
    "                \"price\",\"approximate_quote_24h_volume\"]\n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Filter to USD spot & tradable\n",
    "    filt = (\n",
    "        (df[\"product_type\"] == \"SPOT\") &\n",
    "        (df[\"quote_currency_id\"] == CANON_QUOTE) &\n",
    "        (df[\"status\"] == \"online\") &\n",
    "        (~df[\"trading_disabled\"]) &\n",
    "        (~df[\"is_disabled\"]) &\n",
    "        (~df[\"view_only\"]) &\n",
    "        (~df[\"cancel_only\"]) &\n",
    "        (~df[\"auction_mode\"])\n",
    "    )\n",
    "    df = df[filt]\n",
    "\n",
    "    df[\"asof_date\"] = pd.to_datetime(asof).date()\n",
    "\n",
    "    if save:\n",
    "        out = PRODUCTS_DIR / f\"{asof}_prod.parquet\"\n",
    "        df.to_parquet(out, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0e9841a-3c68-4b8f-b66a-4b7dffd006ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to the difference in products between to saved Parquet files\n",
    "def products_diff(prev_path, curr_path):\n",
    "    \n",
    "    prev = pd.read_parquet(prev_path)\n",
    "    curr = pd.read_parquet(curr_path)\n",
    "    prev_set = set(prev[\"ticker\"])\n",
    "    curr_set = set(curr[\"ticker\"])\n",
    "    adds = sorted(list(curr_set - prev_set))\n",
    "    drops = sorted(list(prev_set - curr_set))\n",
    "    \n",
    "    return {\"adds\": adds, \"drops\": drops}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48a49e18-35ec-43e3-8dee-2cbbabeadd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get OHLC data for each coin\n",
    "def get_coinbase_candle_data(client, product_id, start_date, end_date):\n",
    "    \"\"\"Return a daily OHLCV DataFrame indexed by date. Empty DF if no data.\"\"\"\n",
    "\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    end_date   = pd.Timestamp(end_date)\n",
    "    start_timestamp = int(pd.Timestamp(start_date).timestamp())\n",
    "    end_timestamp = int(pd.Timestamp(end_date).timestamp())\n",
    "\n",
    "    resp = client.get_candles(\n",
    "        product_id=product_id,\n",
    "        start=start_timestamp,\n",
    "        end=end_timestamp,\n",
    "        granularity='ONE_DAY',\n",
    "    )\n",
    "    candles = resp.candles or []\n",
    "\n",
    "    if not candles:\n",
    "        # return an empty frame with the expected schema\n",
    "        cols = ['low','high','open','close','volume']\n",
    "        return pd.DataFrame(columns=cols).astype({c:'float64' for c in cols})\n",
    "\n",
    "    rows = [{\n",
    "        'date':   c['start'],\n",
    "        'low':    float(c['low']),\n",
    "        'high':   float(c['high']),\n",
    "        'open':   float(c['open']),\n",
    "        'close':  float(c['close']),\n",
    "        'volume': float(c['volume']),\n",
    "    } for c in candles]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['date'] = pd.to_datetime(pd.to_numeric(df['date'], errors='coerce'), unit='s', utc=True).dt.date\n",
    "    \n",
    "    return df.sort_values('date').set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44f3ce12-d492-4f08-a7f4-a50a38ccbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to save a list of all eligible products based on their ADV, High-Low Spread and Warmup Day Availability on a MONTHLY basis\n",
    "## Check if enough history is available and if the liquidity metrics meet all required thresholds\n",
    "def has_warmup_coverage(client, product_id: str, asof_date, warmup_days: int) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if there is at least one daily candle on or before (asof - warmup_days),\n",
    "    using a tiny 1-day query window.\n",
    "    \"\"\"\n",
    "    # asof_date can be 'YYYY-MM-DD', date, or datetime\n",
    "    asof = pd.Timestamp(asof_date).date()\n",
    "\n",
    "    # boundary day at 00:00:00 UTC\n",
    "    start = asof - pd.Timedelta(days=warmup_days)\n",
    "    end   = start + pd.Timedelta(days=1)\n",
    "    start_timestamp = int(pd.Timestamp(start).timestamp())\n",
    "    end_timestamp = int(pd.Timestamp(end).timestamp())\n",
    "\n",
    "    resp = client.get_candles(\n",
    "        product_id=product_id,\n",
    "        start=start_timestamp,\n",
    "        end=end_timestamp,\n",
    "        granularity=\"ONE_DAY\",   # required enum value\n",
    "    )\n",
    "    candles = getattr(resp, \"candles\", []) or []\n",
    "    return bool(candles)\n",
    "\n",
    "\n",
    "def get_liquidity_metrics(client, product_id, asof_date, lookback_day_count=90):\n",
    "    \n",
    "    end_date = pd.Timestamp(asof_date).date()\n",
    "    start_date = end_date - pd.Timedelta(days=lookback_day_count)\n",
    "    df = get_coinbase_candle_data(client, product_id=product_id, start_date=start_date, end_date=end_date)\n",
    "    df['notional_usd'] = df['volume'] * df['close']\n",
    "    df['adv_90d_median'] = df['notional_usd'].rolling(90).median()\n",
    "    df['high_low_spread_bps'] = (df['high'] - df['low']) / ((df['high'] + df['low']) / 2) * 10000\n",
    "    df['high_low_spread_90d_median'] = df['high_low_spread_bps'].rolling(90).median()\n",
    "\n",
    "    return df\n",
    "\n",
    "## Get liquidity metrics for all coins\n",
    "def get_liquidity_metrics_all_tickers_monthly(client, product_id_list, asof_date, lookback_day_count=90, warmup_days=300, save=True):\n",
    "\n",
    "    df_liquidity = pd.DataFrame(columns=['asof_date','product_id','adv_90d_median','high_low_spread_90d_median','warmup_days_available'])\n",
    "    \n",
    "    for product_id in product_id_list:\n",
    "        try:\n",
    "            df = get_liquidity_metrics(client, product_id, asof_date=asof_date, lookback_day_count=lookback_day_count)\n",
    "            row = {\n",
    "                'asof_date': asof_date,\n",
    "                'product_id': product_id,\n",
    "                'adv_90d_median': df.loc[pd.Timestamp(asof_date).date()]['adv_90d_median'],\n",
    "                'high_low_spread_90d_median': df.loc[pd.Timestamp(asof_date).date()]['high_low_spread_90d_median'],\n",
    "                'warmup_days_available': has_warmup_coverage(client, product_id=product_id, asof_date=asof_date, warmup_days=warmup_days)\n",
    "            }\n",
    "            df_liquidity.loc[df_liquidity.shape[0]] = row\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    if save:\n",
    "        out = LIQUIDITY_DIR / f\"{asof_date}_liquidity.parquet\"\n",
    "        df_liquidity.to_parquet(out, index=False)\n",
    "\n",
    "    return df_liquidity\n",
    "\n",
    "## Get a list of all eligible coins from all the coins based on liquidity requirements\n",
    "def get_eligible_ticker_list_monthly(df, asof_date, median_adv_col='adv_90d_median', median_high_low_spread_col='high_low_spread_90d_median', \n",
    "                                     warmup_days_col='warmup_days_available', adv_quantile_threshold=0.60, high_low_quantile_threshold=0.60, save=True):\n",
    "\n",
    "    ## Get ADV Floor\n",
    "    adv_null_cond = (df[median_adv_col].notnull())\n",
    "    adv_usd_floor = np.quantile(df[adv_null_cond][median_adv_col], q=adv_quantile_threshold)\n",
    "\n",
    "    ## Get High-Low Spread Floor\n",
    "    high_low_null_cond = (df[median_high_low_spread_col].notnull())\n",
    "    high_low_spread_floor = np.quantile(df[high_low_null_cond][median_high_low_spread_col], q=high_low_quantile_threshold)\n",
    "\n",
    "    ## Exclude Stablecoins\n",
    "    exclusions = ['USDC-USD', 'DAI-USD', 'USDT-USD']\n",
    "\n",
    "    ## Create eligibility criteria\n",
    "    eligible_cond = (\n",
    "        (df[warmup_days_col]) &\n",
    "        (df[median_adv_col] >= adv_usd_floor) &\n",
    "        (df[median_high_low_spread_col] <= high_low_spread_floor) &\n",
    "        (~df['product_id'].isin(exclusions))\n",
    "    )\n",
    "\n",
    "    ## Create eligibility ticker list\n",
    "    df_eligible = df[eligible_cond].reset_index(drop=True)\n",
    "\n",
    "    if save:\n",
    "        out = ELIGIBLE_DIR / f\"{asof_date}_eligible.parquet\"\n",
    "        df_eligible.to_parquet(out, index=False)\n",
    "\n",
    "    return df_eligible, adv_usd_floor, high_low_spread_floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f9f83e7-f966-4a55-af0d-4253140cdbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to get all available coins that meet the ADV and High-Low Sprad criteria on a DAILY basis\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- worker: minimal jitter + tiny retry on 429, nothing else ----\n",
    "def _call_with_retry(fn, *args, **kwargs):\n",
    "    for i in range(2):  # at most 2 tries\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            status = getattr(getattr(e, \"response\", None), \"status_code\", None)\n",
    "            if status == 429:\n",
    "                # quick backoff (200ms, then 400ms) + jitter\n",
    "                time.sleep(0.2 * (2**i) + random.uniform(0.0, 0.08))\n",
    "                continue\n",
    "            raise\n",
    "    # last attempt without catching\n",
    "    return fn(*args, **kwargs)\n",
    "\n",
    "def _one_product_liquidity(client, pid, asof_date, lookback_day_count):\n",
    "    asof_key = pd.Timestamp(asof_date).date()\n",
    "\n",
    "    # a hair of jitter so batch submissions don't land at the same millisecond\n",
    "    # (very small; won't slow total runtime noticeably)\n",
    "    time.sleep(random.uniform(0.0, 0.02))\n",
    "\n",
    "    metrics = _call_with_retry(\n",
    "        get_liquidity_metrics,\n",
    "        client=client, product_id=pid, asof_date=asof_key, lookback_day_count=lookback_day_count\n",
    "    )\n",
    "    if metrics is None or metrics.empty:\n",
    "        return None\n",
    "\n",
    "    if asof_key not in metrics.index:\n",
    "        metrics = metrics.loc[:asof_key]\n",
    "        if metrics.empty:\n",
    "            return None\n",
    "\n",
    "    row = metrics.iloc[-1]\n",
    "    adv = row.get(\"adv_90d_median\", np.nan)\n",
    "    hls = row.get(\"high_low_spread_90d_median\", np.nan)\n",
    "    if not np.isfinite(adv) or not np.isfinite(hls):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"asof_date\": asof_key,\n",
    "        \"product_id\": pid,\n",
    "        \"adv_90d_median\": float(adv),\n",
    "        \"high_low_spread_90d_median\": float(hls),\n",
    "    }\n",
    "\n",
    "# ---- simple batched submit: keeps it fast but smooth ----\n",
    "def _batched(seq, n):\n",
    "    for i in range(0, len(seq), n):\n",
    "        yield seq[i:i+n]\n",
    "\n",
    "def get_liquidity_metrics_all_tickers_daily_fast(\n",
    "    client, product_id_list, asof_date,\n",
    "    lookback_day_count=90, \n",
    "    max_workers=12, batch_size=16, between_batches_sleep=0.10, save=True\n",
    "):\n",
    "    STABLECOINS = {'USDC-USD','DAI-USD','USDT-USD'}\n",
    "    # keep simple upfront pruning\n",
    "    pids = [p for p in product_id_list if p.endswith(\"-USD\") and p not in STABLECOINS]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # --- batched submission: prevents huge burst that causes 429 ---\n",
    "    for batch in _batched(pids, batch_size):\n",
    "        # small, short-lived pool per batch keeps scheduling overhead low\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            # Use executor.map for low overhead (faster than as_completed here)\n",
    "            args_iter = ((client, pid, asof_date, lookback_day_count) for pid in batch)\n",
    "            for res in ex.map(lambda a: _one_product_liquidity(*a), args_iter, chunksize=1):\n",
    "                if res:\n",
    "                    rows.append(res)\n",
    "\n",
    "        # tiny pause between bursts; tune 0.05â€“0.15s if you still see 429s\n",
    "        if between_batches_sleep:\n",
    "            time.sleep(between_batches_sleep)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        \"asof_date\",\"product_id\",\"adv_90d_median\",\"high_low_spread_90d_median\"\n",
    "    ])\n",
    "    # if save and len(df):\n",
    "    #     out = LIQUIDITY_DIR / f\"{pd.Timestamp(asof_date).date()}_liquidity.parquet\"\n",
    "    #     df.to_parquet(out, index=False)\n",
    "    return df\n",
    "\n",
    "## Get a list of all eligible coins from all the coins based on liquidity requirements\n",
    "def get_eligible_ticker_list_daily(df, asof_date, median_adv_col='adv_90d_median', median_high_low_spread_col='high_low_spread_90d_median', \n",
    "                                   adv_quantile_threshold=0.60, high_low_quantile_threshold=0.60, save=True):\n",
    "\n",
    "    ## Get ADV Floor\n",
    "    adv_null_cond = (df[median_adv_col].notnull())\n",
    "    adv_usd_floor = np.quantile(df[adv_null_cond][median_adv_col], q=adv_quantile_threshold)\n",
    "\n",
    "    ## Get High-Low Spread Floor\n",
    "    high_low_null_cond = (df[median_high_low_spread_col].notnull())\n",
    "    high_low_spread_floor = np.quantile(df[high_low_null_cond][median_high_low_spread_col], q=high_low_quantile_threshold)\n",
    "\n",
    "    ## Exclude Stablecoins\n",
    "    STABLECOINS = ['USDC-USD', 'DAI-USD', 'USDT-USD']\n",
    "\n",
    "    ## Create eligibility criteria\n",
    "    eligible_cond = (\n",
    "        (df[median_adv_col] >= adv_usd_floor) &\n",
    "        (df[median_high_low_spread_col] <= high_low_spread_floor) &\n",
    "        (~df['product_id'].isin(STABLECOINS))\n",
    "    )\n",
    "\n",
    "    ## Create eligibility ticker list\n",
    "    df_eligible = df[eligible_cond].reset_index(drop=True)\n",
    "\n",
    "    if save:\n",
    "        out = ELIGIBLE_DIR / f\"{asof_date}_eligible.parquet\"\n",
    "        df_eligible.to_parquet(out, index=False)\n",
    "\n",
    "    return df_eligible, adv_usd_floor, high_low_spread_floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3eae7-dd00-4e51-98b2-98c9311d00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Code to get the monthly snapshot of all coins on Coinbase that meet the eligibility criteria (ADV, High-Low Spread and Warmup Period)\n",
    "lookback_day_count = 90\n",
    "warmup_days = cfg['run']['warmup_days']\n",
    "adv_quantile = 0.60\n",
    "high_low_quantile = 0.60\n",
    "date_range = pd.date_range(start=pd.Timestamp('2022-03-01').date(), end=pd.Timestamp('2025-07-31').date(), freq='M')\n",
    "client = cn.get_coinbase_rest_api_client(portfolio_name='Default')\n",
    "for date in date_range:\n",
    "    print(date)\n",
    "    df_products = coinbase_product_snapshot(client, asof=date.date(), save=True)\n",
    "    curr_product_list = df_products.product_id.unique().tolist()\n",
    "    df_liquidity_ticker = get_liquidity_metrics_all_tickers_monthly(client, product_id_list=curr_product_list, asof_date=date.date(),\n",
    "                                                                    lookback_day_count=lookback_day_count, warmup_days=warmup_days, save=True)\n",
    "    df_eligible, adv_usd_floor, high_low_spread_floor = get_eligible_ticker_list_monthly(df=df_liquidity_ticker, asof_date=date.date(), median_adv_col='adv_90d_median',\n",
    "                                                                                         median_high_low_spread_col='high_low_spread_90d_median', \n",
    "                                                                                         warmup_days_col='warmup_days_available',\n",
    "                                                                                         adv_quantile_threshold=adv_quantile, high_low_quantile_threshold=high_low_quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22adbce-778b-4a1c-82ce-afadc4e44a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Code to get the daily snapshot of all coins on Coinbase that meet the eligibility criteria (ADV and High-Low Spread)\n",
    "lookback_day_count = 90\n",
    "warmup_days = cfg['run']['warmup_days']\n",
    "adv_quantile = 0.60\n",
    "high_low_quantile = 0.60\n",
    "date_range = pd.date_range(start=pd.Timestamp('2022-03-01').date(), end=pd.Timestamp('2025-07-31').date(), freq='M')\n",
    "# for date in date_range:\n",
    "date = pd.Timestamp('2025-11-03')\n",
    "print(date)\n",
    "df_products = coinbase_product_snapshot(client, asof=date.date(), save=True)\n",
    "curr_product_list = df_products.product_id.unique().tolist()\n",
    "df_liquidity_ticker_fast = get_liquidity_metrics_all_tickers_daily_fast(client, product_id_list=curr_product_list, asof_date=date.date(),\n",
    "                                                                        lookback_day_count=lookback_day_count, max_workers=5, batch_size=12, between_batches_sleep=0.10, save=False)\n",
    "df_eligible_test, adv_usd_floor, high_low_spread_floor = get_eligible_ticker_list_daily(df=df_liquidity_ticker_fast, asof_date=date.date(), median_adv_col='adv_90d_median',\n",
    "                                                                                        median_high_low_spread_col='high_low_spread_90d_median', \n",
    "                                                                                        adv_quantile_threshold=adv_quantile, high_low_quantile_threshold=high_low_quantile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae348b7-3e3a-45f2-80f8-95964c1b8c16",
   "metadata": {},
   "source": [
    "## Prod Config Override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "263a8bef-0415-436a-ab42-f19c9b8c3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_confirm_days = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7d5fd-e86b-4a9c-9fc7-e969798599c7",
   "metadata": {},
   "source": [
    "## Expanded Universe Trend Following Trade Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b3f2017-9fdb-4894-9ed3-f23dc4ea716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to generate Trend Following Signal with Moving Avg Crossover, Donchian Channel, Rolling R Squared and Vol of Vol Signals\n",
    "def get_trend_donchian_signal_for_portfolio_with_rolling_r_sqr_vol_of_vol(\n",
    "        start_date, end_date, ticker_list, fast_mavg, slow_mavg, mavg_stepsize, mavg_z_score_window,\n",
    "        entry_rolling_donchian_window, exit_rolling_donchian_window, use_donchian_exit_gate,\n",
    "        ma_crossover_signal_weight, donchian_signal_weight, weighted_signal_ewm_window,\n",
    "        rolling_r2_window=30, lower_r_sqr_limit=0.2, upper_r_sqr_limit=0.8, r2_smooth_window=3, r2_confirm_days=0,\n",
    "        log_std_window=14, coef_of_variation_window=30, vol_of_vol_z_score_window=252, vol_of_vol_p_min=0.6,\n",
    "        r2_strong_threshold=0.8, use_activation=True, tanh_activation_constant_dict=None, moving_avg_type='exponential',\n",
    "        long_only=False, price_or_returns_calc='price', use_coinbase_data=True, use_saved_files=True,\n",
    "        saved_file_end_date='2025-07-31'):\n",
    "\n",
    "    ## Generate trend signal for all tickers\n",
    "    trend_list = []\n",
    "    date_list = cn.coinbase_start_date_by_ticker_dict\n",
    "\n",
    "    for ticker in ticker_list:\n",
    "        # Create Column Names\n",
    "        trend_continuous_signal_col = f'{ticker}_mavg_ribbon_slope'\n",
    "        trend_continuous_signal_rank_col = f'{ticker}_mavg_ribbon_rank'\n",
    "        final_signal_col = f'{ticker}_final_signal'\n",
    "        close_price_col = f'{ticker}_close'\n",
    "        open_price_col = f'{ticker}_open'\n",
    "        rolling_r2_col = f'{ticker}_rolling_r_sqr'\n",
    "        final_weighted_additive_signal_col = f'{ticker}_final_weighted_additive_signal'\n",
    "\n",
    "        # if pd.to_datetime(date_list[ticker]).date() > start_date:\n",
    "        #     run_date = pd.to_datetime(date_list[ticker]).date()\n",
    "        # else:\n",
    "        #     run_date = start_date\n",
    "\n",
    "        df_trend = tf.generate_trend_signal_with_donchian_channel_continuous_with_rolling_r_sqr_vol_of_vol(\n",
    "            start_date=start_date, end_date=end_date, ticker=ticker, fast_mavg=fast_mavg, slow_mavg=slow_mavg,\n",
    "            mavg_stepsize=mavg_stepsize, mavg_z_score_window=mavg_z_score_window,\n",
    "            entry_rolling_donchian_window=entry_rolling_donchian_window,\n",
    "            exit_rolling_donchian_window=exit_rolling_donchian_window, use_donchian_exit_gate=use_donchian_exit_gate,\n",
    "            ma_crossover_signal_weight=ma_crossover_signal_weight, donchian_signal_weight=donchian_signal_weight,\n",
    "            weighted_signal_ewm_window=weighted_signal_ewm_window,\n",
    "            rolling_r2_window=rolling_r2_window, lower_r_sqr_limit=lower_r_sqr_limit,\n",
    "            upper_r_sqr_limit=upper_r_sqr_limit, r2_smooth_window=r2_smooth_window, r2_confirm_days=r2_confirm_days,\n",
    "            log_std_window=log_std_window, coef_of_variation_window=coef_of_variation_window,\n",
    "            vol_of_vol_z_score_window=vol_of_vol_z_score_window, vol_of_vol_p_min=vol_of_vol_p_min,\n",
    "            r2_strong_threshold=r2_strong_threshold,\n",
    "            use_activation=use_activation, tanh_activation_constant_dict=tanh_activation_constant_dict,\n",
    "            moving_avg_type=moving_avg_type, price_or_returns_calc=price_or_returns_calc, long_only=long_only,\n",
    "            use_coinbase_data=use_coinbase_data, use_saved_files=use_saved_files,\n",
    "            saved_file_end_date=saved_file_end_date)\n",
    "\n",
    "        trend_cols = [close_price_col, open_price_col, trend_continuous_signal_col, trend_continuous_signal_rank_col,\n",
    "                      final_weighted_additive_signal_col,\n",
    "                      rolling_r2_col, final_signal_col]\n",
    "        df_trend = df_trend[trend_cols]\n",
    "        trend_list.append(df_trend)\n",
    "\n",
    "    df_trend = pd.concat(trend_list, axis=1)\n",
    "\n",
    "    return df_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31eff185-0d12-4bf7-ac30-015f74460ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to Clamp the Per Asset Cap\n",
    "def clamp_to_per_asset_caps(desired_delta: pd.Series,\n",
    "                            current_notional_prev: pd.Series,\n",
    "                            total_portfolio_value_upper_limit: float,\n",
    "                            per_asset_weight_cap: float,\n",
    "                            per_asset_notional_cap: float | None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Enforce per-asset caps by limiting *post-trade* notional to:\n",
    "      min(weight_cap * TPV_upper, absolute_cap if given).\n",
    "    Only constrains the *upside* (you can always reduce).\n",
    "    \"\"\"\n",
    "    if len(desired_delta) == 0:\n",
    "        return desired_delta\n",
    "\n",
    "    # cap each asset's *final* notional (current + delta)\n",
    "    weight_cap_notional = per_asset_weight_cap * float(total_portfolio_value_upper_limit)\n",
    "    cap_notional = weight_cap_notional if per_asset_notional_cap is None else min(weight_cap_notional, per_asset_notional_cap)\n",
    "\n",
    "    # current long notionals; if you allow shorting, drop the clip\n",
    "    curr = current_notional_prev.fillna(0.0).clip(lower=0.0)\n",
    "    target_after = (curr + desired_delta).fillna(0.0)\n",
    "\n",
    "    # where target_after exceeds cap, cut the delta so final == cap\n",
    "    over = target_after > cap_notional\n",
    "    if over.any():\n",
    "        desired_delta.loc[over] = cap_notional - curr.loc[over]\n",
    "\n",
    "    return desired_delta\n",
    "\n",
    "## Code to enforce a freeze on position increases on any frozen tickers if chosen (allow_adds_on_frozen: True/False)\n",
    "def enforce_freeze_on_notional_deltas(desired_delta: pd.Series,\n",
    "                                      current_notional_prev: pd.Series,\n",
    "                                      frozen_tickers: list[str] | None,\n",
    "                                      allow_adds_on_frozen: bool) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Freeze policy:\n",
    "      - If allow_adds_on_frozen=True: do nothing (you may increase frozen tickers).\n",
    "      - If False: you may *not* increase frozen tickers; only flat or decrease.\n",
    "    \"\"\"\n",
    "    if not frozen_tickers:\n",
    "        return desired_delta\n",
    "\n",
    "    if allow_adds_on_frozen:\n",
    "        return desired_delta\n",
    "\n",
    "    # block positive deltas for frozen tickers\n",
    "    fz = pd.Index(frozen_tickers).intersection(desired_delta.index)\n",
    "    pos = desired_delta.loc[fz] > 0\n",
    "    if pos.any():\n",
    "        desired_delta.loc[fz[pos]] = 0.0\n",
    "    return desired_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22c8a29c-a9e9-4d49-adc8-079e96a0c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to get the Trend Following Final Signal for all coins on a given date\n",
    "def _get_today_signals(df: pd.DataFrame, date, tickers, suffix: str) -> pd.Series:\n",
    "    \"\"\"Return a Series index=ticker, value=today's signal (NaN -> -inf for ranking).\"\"\"\n",
    "    cols = [f\"{t}{suffix}\" for t in tickers]\n",
    "    row = df.loc[date, cols] if date in df.index else pd.Series([np.nan]*len(cols), index=cols)\n",
    "    s = row.copy()\n",
    "    s.index = [c[:-len(suffix)] for c in s.index]\n",
    "    return s.astype(float).fillna(-np.inf)\n",
    "\n",
    "## Code to manage the daily position budget based on if the max number of positions is set or\n",
    "## if there is a limit on the maximum number of new positions that can be opened on any given day\n",
    "def _enforce_daily_position_budget(\n",
    "    desired_delta: pd.Series,\n",
    "    current_notional_prev: pd.Series,\n",
    "    today_signals: pd.Series,\n",
    "    max_positions: int | None,\n",
    "    max_new_per_day: int | None,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Allows exits/reductions as-is. Among *entries* (delta>0 on names not currently held),\n",
    "    keep only top-K by signal, where K = min(max_new_per_day, max_positions - held_count).\n",
    "    \"\"\"\n",
    "    ## Make no changes if there is no set max positions or new max positions per day and execute desired trades\n",
    "    if max_positions is None and max_new_per_day is None:\n",
    "        return desired_delta\n",
    "\n",
    "    ## Get the total number of held positions in the portfolio and the number of desired new entrants\n",
    "    curr = current_notional_prev.fillna(0.0)\n",
    "    held = curr > 0\n",
    "    entrants = (desired_delta.fillna(0.0) > 0) & (~held)\n",
    "\n",
    "    ## If there are no new entrants, do nothing and execute the desired trades\n",
    "    if not entrants.any():\n",
    "        return desired_delta\n",
    "\n",
    "    ## If max number of positions is set, get the number of open slots in the portfolio\n",
    "    held_cnt = int(held.sum())\n",
    "    slots = None\n",
    "    if max_positions is not None:\n",
    "        slots = max(max_positions - held_cnt, 0)\n",
    "    \n",
    "    ## If there is a max number of new positions set, modify the number of slots based on the max_new_per_day param\n",
    "    if max_new_per_day is not None:\n",
    "        slots = max_new_per_day if slots is None else min(slots, max_new_per_day)\n",
    "\n",
    "    ## If the max number of positions is not exceeded after accounting for the entrants, execute desired trades\n",
    "    if slots is None or slots >= int(entrants.sum()):\n",
    "        return desired_delta  # nothing to trim\n",
    "\n",
    "    ## If the number of entrants is greater than the number of available slots, use signal strength to \n",
    "    ## rank the new entrants and pick the top few that fill up the number of open slots\n",
    "    tf_signal = today_signals.reindex(desired_delta.index).fillna(-np.inf)\n",
    "    tf_signal_entrants = tf_signal[entrants].sort_values(ascending=False)\n",
    "    allow = set(tf_signal_entrants.index[:slots])\n",
    "\n",
    "    ## Based on the signal strength, block the entrants that are not chosen\n",
    "    block = [t for t in tf_signal_entrants.index if t not in allow]\n",
    "    if block:\n",
    "        desired_delta.loc[block] = 0.0\n",
    "        \n",
    "    return desired_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dfb5ee75-a0e2-4dca-9c3f-dd16281568d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to get the daily position sizes based on the target volatility\n",
    "def get_target_volatility_daily_portfolio_positions_expanded_universe(df, ticker_list, initial_capital, rolling_cov_window,\n",
    "                                                                      stop_loss_strategy, rolling_atr_window, atr_multiplier,\n",
    "                                                                      highest_high_window, cash_buffer_percentage,\n",
    "                                                                      annualized_target_volatility, transaction_cost_est=0.001,\n",
    "                                                                      passive_trade_rate=0.05, notional_threshold_pct=0.02,\n",
    "                                                                      min_trade_notional_abs=10, cooldown_counter_threshold=3,\n",
    "                                                                      annual_trading_days=365, use_specific_start_date=False,\n",
    "                                                                      signal_start_date=None, start_date=None, previous_month_open_positions_df=None,\n",
    "                                                                      frozen_ticker_list=None,\n",
    "                                                                      per_asset_weight_cap=0.25,                # e.g., max 25% of PV upper limit per asset\n",
    "                                                                      per_asset_notional_cap=None,              # e.g., 5000. If None, ignore absolute cap.\n",
    "                                                                      allow_adds_on_frozen=True,                # True = you can scale UP frozen tickers\n",
    "                                                                      eligibility_by_date=None,                 # dict[date]->set([...]) of liquidity-eligible tickers\n",
    "                                                                      force_exit_if_ineligible=True,            # True = immediate full exit if ineligible today\n",
    "                                                                      max_positions = None,                     # e.g., 12\n",
    "                                                                      max_new_per_day = None,                   # e.g., 3\n",
    "                                                                      signal_col_suffix = \"_final_signal\"       # so we can rank entries cleanly\n",
    "                                                                     ):\n",
    "\n",
    "    # Ensure DatetimeIndex (tz-naive), normalized, sorted\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index, utc=True).tz_localize(None)\n",
    "    elif df.index.tz is not None:\n",
    "        df.index = df.index.tz_localize(None)\n",
    "    df.index = df.index.normalize()\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    ## Calculate the covariance matrix for tickers in the portfolio\n",
    "    returns_cols = [f'{ticker}_t_1_close_pct_returns' for ticker in ticker_list]\n",
    "    cov_matrix = df[returns_cols].rolling(rolling_cov_window).cov(pairwise=True).dropna()\n",
    "\n",
    "    ## Delete rows prior to the first available date of the covariance matrix\n",
    "    cov_matrix_start_date = cov_matrix.index[0][0]\n",
    "    df = df[df.index >= cov_matrix_start_date]\n",
    "\n",
    "    ## Derive the Daily Target Portfolio Volatility\n",
    "    daily_target_volatility = annualized_target_volatility / np.sqrt(annual_trading_days)\n",
    "\n",
    "    ## Reorder dataframe columns\n",
    "    for ticker in ticker_list:\n",
    "        df[f'{ticker}_new_position_size'] = 0.0\n",
    "        df[f'{ticker}_new_position_notional'] = 0.0\n",
    "        df[f'{ticker}_open_position_size'] = 0.0\n",
    "        df[f'{ticker}_open_position_notional'] = 0.0\n",
    "        df[f'{ticker}_actual_position_size'] = 0.0\n",
    "        df[f'{ticker}_actual_position_notional'] = 0.0\n",
    "        df[f'{ticker}_short_sale_proceeds'] = 0.0\n",
    "        df[f'{ticker}_new_position_entry_exit_price'] = 0.0\n",
    "        df[f'{ticker}_target_vol_normalized_weight'] = 0.0\n",
    "        df[f'{ticker}_target_notional'] = 0.0\n",
    "        df[f'{ticker}_target_size'] = 0.0\n",
    "        df[f'{ticker}_stop_loss'] = 0.0\n",
    "        df[f'{ticker}_stopout_flag'] = False\n",
    "        df[f'{ticker}_cooldown_counter'] = 0.0\n",
    "        df[f'{ticker}_event'] = np.nan\n",
    "    ord_cols = size_bin.reorder_columns_by_ticker(df.columns, ticker_list)\n",
    "    df = df[ord_cols]\n",
    "\n",
    "    ## Portfolio Level Cash and Positions are all set to 0\n",
    "    df['daily_portfolio_volatility'] = 0.0\n",
    "    df['available_cash'] = 0.0\n",
    "    df['count_of_positions'] = 0.0\n",
    "    df['total_actual_position_notional'] = 0.0\n",
    "    df['total_target_notional'] = 0.0\n",
    "    df['total_portfolio_value'] = 0.0\n",
    "    df['total_portfolio_value_upper_limit'] = 0.0\n",
    "    df['target_vol_scaling_factor'] = 1.0\n",
    "    df['cash_scaling_factor'] = 1.0\n",
    "    df['final_scaling_factor'] = 1.0\n",
    "    df['cash_shrink_factor'] = 1.0\n",
    "\n",
    "    # --- pick the desired logical start date for THIS run ---\n",
    "    if use_specific_start_date and signal_start_date is not None:\n",
    "        logical_start = pd.Timestamp(signal_start_date).normalize()   # only for first month\n",
    "    elif start_date is not None:\n",
    "        logical_start = pd.Timestamp(start_date).normalize()          # monthâ€™s first day\n",
    "    else:\n",
    "        logical_start = df.index[0]                                   # fallback\n",
    "    \n",
    "    # --- align to available index after the cov trimming you did above ---\n",
    "    # df already trimmed by cov_matrix_start_date; its first index may be > logical_start\n",
    "    pos = df.index.searchsorted(logical_start, side=\"left\")\n",
    "    start_index_position = int(min(pos, len(df.index)-1))\n",
    "\n",
    "    ## TODO: ADD PREVIOUS MONTH OPEN POSITIONS, AVAILABLE CASH AND TOTAL PORTFOLIO VALUE HERE\n",
    "    if previous_month_open_positions_df is not None:\n",
    "        df.iloc[start_index_position, df.columns.get_loc('available_cash')]        = previous_month_open_positions_df['available_cash'].iloc[0]\n",
    "        df.iloc[start_index_position, df.columns.get_loc('total_portfolio_value')] = previous_month_open_positions_df['total_portfolio_value'].iloc[0]\n",
    "        prev_month_open_ticker_list = [col.split('_')[0] for col in previous_month_open_positions_df.columns if '_actual_position_notional' in col]\n",
    "        for ticker in prev_month_open_ticker_list:\n",
    "            df.iloc[start_index_position, df.columns.get_loc(f'{ticker}_actual_position_notional')] = previous_month_open_positions_df[f'{ticker}_actual_position_notional'].iloc[0]\n",
    "            df.iloc[start_index_position, df.columns.get_loc(f'{ticker}_actual_position_size')]     = previous_month_open_positions_df[f'{ticker}_actual_position_size'].iloc[0]\n",
    "    else:\n",
    "        print('Previous Month End Not Available')\n",
    "        df.iloc[start_index_position, df.columns.get_loc('available_cash')]        = initial_capital\n",
    "        df.iloc[start_index_position, df.columns.get_loc('total_portfolio_value')] = initial_capital\n",
    "\n",
    "    ## Identify Daily Positions starting from day 2\n",
    "    for date in df.index[start_index_position + 1:]:\n",
    "        previous_date = df.index[df.index.get_loc(date) - 1]\n",
    "\n",
    "        ## Start the day with the available cash from yesterday\n",
    "        df.loc[date, 'available_cash'] = df.loc[previous_date, 'available_cash']\n",
    "\n",
    "        ## Roll Portfolio Value from the Previous Day\n",
    "        total_portfolio_value = df.loc[previous_date, 'total_portfolio_value']\n",
    "        df.loc[date, 'total_portfolio_value'] = total_portfolio_value\n",
    "\n",
    "        ## Update Total Portfolio Value Upper Limit based on the Total Portfolio Value\n",
    "        total_portfolio_value_upper_limit = (df.loc[date, 'total_portfolio_value'] *\n",
    "                                             (1 - cash_buffer_percentage))\n",
    "        df.loc[date, 'total_portfolio_value_upper_limit'] = total_portfolio_value_upper_limit\n",
    "\n",
    "        ## Calculate the target notional by ticker\n",
    "        df = size_cont.get_target_volatility_position_sizing(df, cov_matrix, date, ticker_list, daily_target_volatility,\n",
    "                                                             total_portfolio_value_upper_limit)\n",
    "\n",
    "        ## Adjust Positions for Cash Available\n",
    "        desired_positions, cash_shrink_factor = size_cont.get_cash_adjusted_desired_positions(\n",
    "            df, date, previous_date, ticker_list, cash_buffer_percentage, transaction_cost_est, passive_trade_rate,\n",
    "            total_portfolio_value, notional_threshold_pct, min_trade_notional_abs)\n",
    "\n",
    "        ## Build current notional *from previous_date* for the union ticker_list\n",
    "        current_notional_prev = df.loc[previous_date, [f\"{t}_actual_position_notional\" for t in ticker_list]]\n",
    "        current_notional_prev.index = [c.replace(\"_actual_position_notional\",\"\") for c in current_notional_prev.index]\n",
    "\n",
    "        # Convert your desired_positions dict to a Series keyed by ticker -> notional delta\n",
    "        desired_delta = pd.Series(\n",
    "            {t: desired_positions[t]['new_trade_notional'] for t in desired_positions},\n",
    "            dtype=float\n",
    "        ).reindex(current_notional_prev.index).fillna(0.0)\n",
    "\n",
    "        ## NEW (3): daily liquidity eligibility -> hard exit\n",
    "        if force_exit_if_ineligible and (eligibility_by_date is not None):\n",
    "            ## These include all coins that are in today's ticker list and are eligible\n",
    "            ## This returns the set of common elements between eligibility_by_date and ticker_list\n",
    "            today_set = set(eligibility_by_date.get(pd.Timestamp(date).date(), ticker_list))\n",
    "            drops = [t for t in ticker_list if t not in today_set and float(current_notional_prev.get(t, 0.0)) != 0.0]\n",
    "            if drops:\n",
    "                # Full flatten: set delta = -current_notional\n",
    "                # (If you want a â€œreduce-onlyâ€ when negative PnL, this is where you'd do it; here we just flatten.)\n",
    "                desired_delta.loc[drops] = -current_notional_prev.loc[drops].astype(float)\n",
    "                # optional: mark events (per-ticker columns already exist)\n",
    "                for t in drops:\n",
    "                    df.loc[date, f'{t}_event'] = 'Liquidity Exit'\n",
    "    \n",
    "                # Also block *new* entries for ineligible tickers\n",
    "                blocks = [t for t in ticker_list if t not in today_set and float(current_notional_prev.get(t, 0.0)) == 0.0]\n",
    "                if blocks:\n",
    "                    desired_delta.loc[blocks] = 0.0\n",
    "        \n",
    "        ## NEW (2): freeze policy now *allows* adds if you wish\n",
    "        desired_delta = enforce_freeze_on_notional_deltas(\n",
    "            desired_delta, current_notional_prev,\n",
    "            frozen_tickers=frozen_ticker_list,\n",
    "            allow_adds_on_frozen=allow_adds_on_frozen\n",
    "        )\n",
    "\n",
    "        ## NEW (1): apply per-asset caps to post-trade notionals\n",
    "        desired_delta = clamp_to_per_asset_caps(\n",
    "            desired_delta=desired_delta,\n",
    "            current_notional_prev=current_notional_prev,\n",
    "            total_portfolio_value_upper_limit=total_portfolio_value_upper_limit,\n",
    "            per_asset_weight_cap=per_asset_weight_cap,\n",
    "            per_asset_notional_cap=per_asset_notional_cap\n",
    "        )\n",
    "\n",
    "        # ---- NEW: Daily position budget (entries only) ----\n",
    "        today_signals = _get_today_signals(df, date, ticker_list, signal_col_suffix)\n",
    "        desired_delta = _enforce_daily_position_budget(\n",
    "            desired_delta=desired_delta,\n",
    "            current_notional_prev=current_notional_prev,\n",
    "            today_signals=today_signals,\n",
    "            max_positions=max_positions,\n",
    "            max_new_per_day=max_new_per_day\n",
    "        )\n",
    "        \n",
    "        ## Write back to your dict structure (if your downstream expects the nested dict)\n",
    "        for t in desired_positions:\n",
    "            desired_positions[t]['new_trade_notional'] = float(desired_delta.get(t, 0.0))\n",
    "\n",
    "        ## Get the daily positions\n",
    "        df = size_cont.get_daily_positions_and_portfolio_cash(\n",
    "            df, date, previous_date, desired_positions, cash_shrink_factor, ticker_list,\n",
    "            stop_loss_strategy, rolling_atr_window, atr_multiplier, highest_high_window,\n",
    "            transaction_cost_est, passive_trade_rate, cooldown_counter_threshold)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe2ab159-8747-4af8-a592-eb3e5a37321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to build the eligibility_by_date dictionary that contains a list of all eligible\n",
    "## coins by date for a given month\n",
    "\n",
    "def _prune_usd_products(df_products):\n",
    "    cols = set(df_products.columns)\n",
    "    filt = (df_products[\"quote_currency\"] == \"USD\") if \"quote_currency\" in cols else True\n",
    "    if \"status\" in cols:           filt &= (df_products[\"status\"] == \"online\")\n",
    "    if \"trading_disabled\" in cols: filt &= (~df_products[\"trading_disabled\"])\n",
    "    if \"cancel_only\" in cols:      filt &= (~df_products[\"cancel_only\"])\n",
    "    if \"limit_only\" in cols:       filt &= (~df_products[\"limit_only\"])\n",
    "    if \"post_only\" in cols:        filt &= (~df_products[\"post_only\"])\n",
    "    if \"auction_mode\" in cols:     filt &= (~df_products[\"auction_mode\"])\n",
    "    df = df_products.loc[filt].copy()\n",
    "    stables = {\"USDC-USD\",\"USDT-USD\",\"DAI-USD\"}\n",
    "    return [p for p in df[\"product_id\"].unique().tolist() if p.endswith(\"-USD\") and p not in stables]\n",
    "\n",
    "def build_eligibility_by_date_for_month(\n",
    "    client,\n",
    "    month_start_date, month_end_date,\n",
    "    lookback_day_count=90,          # your current default\n",
    "    adv_quantile=0.60, high_low_quantile=0.60,\n",
    "    product_list=None,\n",
    "    max_workers=8, batch_size=16, between_batches_sleep=0.10\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns dict[date -> set(product_id)] for every calendar day in [month_start_date, month_end_date].\n",
    "    \"\"\"\n",
    "    ms = pd.Timestamp(month_start_date).date()\n",
    "    me = pd.Timestamp(month_end_date).date()\n",
    "\n",
    "    if product_list is None:\n",
    "        # 1) snapshot once at month-start and prune\n",
    "        df_products = coinbase_product_snapshot(client, asof=ms, save=True)\n",
    "        curr_product_list = _prune_usd_products(df_products)\n",
    "    else:\n",
    "        curr_product_list = product_list\n",
    "\n",
    "    elig_by_date = {}\n",
    "\n",
    "    # 2) daily scan across the month using the same pid list\n",
    "    for d in pd.date_range(ms, me, freq=\"D\").date:\n",
    "        print(d)\n",
    "        df_liq = get_liquidity_metrics_all_tickers_daily_fast(\n",
    "            client, product_id_list=curr_product_list, asof_date=d,\n",
    "            lookback_day_count=lookback_day_count,\n",
    "            max_workers=max_workers, batch_size=batch_size,\n",
    "            between_batches_sleep=between_batches_sleep, save=False\n",
    "        )\n",
    "        # compute daily eligibility thresholds and list\n",
    "        df_eligible, adv_floor, spread_floor = get_eligible_ticker_list_daily(\n",
    "            df=df_liq, asof_date=d,\n",
    "            median_adv_col='adv_90d_median',\n",
    "            median_high_low_spread_col='high_low_spread_90d_median',\n",
    "            adv_quantile_threshold=adv_quantile,\n",
    "            high_low_quantile_threshold=high_low_quantile,\n",
    "            save=False\n",
    "        )\n",
    "        elig_by_date[d] = set(df_eligible[\"product_id\"].tolist())\n",
    "\n",
    "    return elig_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86009ce5-9f9e-4e1e-b030-f676278a3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG: storage locations\n",
    "# -----------------------------\n",
    "PERF_STORE_DIR = Path(\"/Users/adheerchauhan/Documents/git/trend_following/data_folder/universe/perf_store\")      # partitioned parquet\n",
    "PERF_STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MONTH_END_OPEN_POSITION_DIR = Path(\"/Users/adheerchauhan/Documents/git/trend_following/data_folder/universe/prev_month_open_positions\")\n",
    "MONTH_END_OPEN_POSITION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "POSITIONS_DIR = Path(\"/Users/adheerchauhan/Documents/git/trend_following/data_folder/universe/positions_daily\")\n",
    "POSITIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ELIGIBLE_DIR = Path(\"/Users/adheerchauhan/Documents/git/trend_following/data_folder/universe/eligible_products\")\n",
    "\n",
    "# -----------------------------\n",
    "# UTILS\n",
    "# -----------------------------\n",
    "def _tz_naive_sorted_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    if getattr(idx, \"tz\", None) is not None:\n",
    "        idx = idx.tz_localize(None)\n",
    "    df.index = idx\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_monthly_performance(\n",
    "    df: pd.DataFrame,\n",
    "    cols: list[str],\n",
    "    month_start: pd.Timestamp,\n",
    "    month_end: pd.Timestamp,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Write ONLY the current month's rows (no warmup leakage) to a partitioned Parquet dataset.\n",
    "    \"\"\"\n",
    "    # 1) normalize + sort\n",
    "    df = df.copy()\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    if getattr(idx, \"tz\", None) is not None:\n",
    "        idx = idx.tz_localize(None)\n",
    "    df.index = idx.normalize()\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # 2) clamp to [month_start, month_end]\n",
    "    ms = pd.Timestamp(month_start).normalize()\n",
    "    me = pd.Timestamp(month_end).normalize()\n",
    "    df = df.loc[(df.index >= ms) & (df.index <= me)]\n",
    "\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # 3) keep only requested cols; add missing as NA\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    for c in cols:\n",
    "        out[c] = df[c] if c in df.columns else pd.NA\n",
    "\n",
    "    # 4) add partitions\n",
    "    out = out.reset_index().rename(columns={\"index\": \"date\"})\n",
    "    out[\"year\"]  = out[\"date\"].dt.year.astype(int)\n",
    "    out[\"month\"] = out[\"date\"].dt.month.astype(int)\n",
    "\n",
    "    # sanity: should be a single month\n",
    "    assert out[[\"year\",\"month\"]].drop_duplicates().shape[0] == 1, \"Leak outside month!\"\n",
    "\n",
    "    # nuke existing partition dir for this month to make the write idempotent\n",
    "    y, m = int(out[\"year\"].iloc[0]), int(out[\"month\"].iloc[0])\n",
    "    part_dir = PERF_STORE_DIR / f\"year={y}\" / f\"month={m}\"\n",
    "    if part_dir.exists():\n",
    "        shutil.rmtree(part_dir)\n",
    "\n",
    "    # 5) append to dataset\n",
    "    out.to_parquet(\n",
    "        PERF_STORE_DIR,\n",
    "        engine=\"pyarrow\",\n",
    "        partition_cols=[\"year\", \"month\"],\n",
    "        index=False,\n",
    "        existing_data_behavior=\"overwrite_or_ignore\",  # fine: no duplicates if you only write the month once\n",
    "    )\n",
    "\n",
    "\n",
    "def snapshot_open_positions_long(df, month_end_date, ticker_list):\n",
    "    d = pd.Timestamp(month_end_date).normalize()\n",
    "    df = _tz_naive_sorted_index(df)\n",
    "\n",
    "    available_cash = float(df.loc[d, \"available_cash\"])\n",
    "    total_pv = float(df.loc[d, \"total_portfolio_value\"])\n",
    "\n",
    "    rows = []\n",
    "    for t in ticker_list:\n",
    "        size_col = f\"{t}_actual_position_size\"\n",
    "        notl_col = f\"{t}_actual_position_notional\"\n",
    "        if size_col in df.columns and notl_col in df.columns:\n",
    "            size_val = float(df.loc[d, size_col])\n",
    "            if size_val > 0:\n",
    "                rows.append({\n",
    "                    \"date\": d, \"ticker\": t,\n",
    "                    \"actual_position_size\": float(df.loc[d, size_col]),\n",
    "                    \"actual_position_notional\": float(df.loc[d, notl_col]),\n",
    "                    \"available_cash\": available_cash,\n",
    "                    \"total_portfolio_value\": total_pv,\n",
    "                })\n",
    "\n",
    "    if not rows:\n",
    "        rows.append({\n",
    "            \"date\": d, \"ticker\": None,\n",
    "            \"actual_position_size\": 0.0, \"actual_position_notional\": 0.0,\n",
    "            \"available_cash\": available_cash, \"total_portfolio_value\": total_pv,\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out[\"year\"]  = out[\"date\"].dt.year.astype(\"int32\")\n",
    "    out[\"month\"] = out[\"date\"].dt.month.astype(\"int32\")\n",
    "\n",
    "    # enforce stable dtypes\n",
    "    out[\"ticker\"] = out[\"ticker\"].astype(\"string\")\n",
    "    for c in [\"actual_position_size\",\"actual_position_notional\",\"available_cash\",\"total_portfolio_value\"]:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "    # --- NEW: replace the partition to keep it idempotent ---\n",
    "    y, m = int(out[\"year\"].iloc[0]), int(out[\"month\"].iloc[0])\n",
    "    part_dir = MONTH_END_OPEN_POSITION_DIR / f\"year={y}\" / f\"month={m}\"\n",
    "    if part_dir.exists():\n",
    "        shutil.rmtree(part_dir)\n",
    "\n",
    "    out.to_parquet(\n",
    "        MONTH_END_OPEN_POSITION_DIR,\n",
    "        engine=\"pyarrow\",\n",
    "        partition_cols=[\"year\",\"month\"],\n",
    "        index=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_daily_positions(df_month, ticker_list, month_start_date, month_end_date):\n",
    "    \"\"\"\n",
    "    df_month is your monthly dataframe that currently contains per-coin columns.\n",
    "    This function melts it to long rows per dateÃ—coin and writes only the month (no warmup).\n",
    "    \"\"\"\n",
    "    # 1) clamp to month\n",
    "    df = df_month.copy()\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    if getattr(idx, \"tz\", None) is not None: idx = idx.tz_localize(None)\n",
    "    df.index = idx.normalize()\n",
    "    ms, me = pd.Timestamp(month_start_date).normalize(), pd.Timestamp(month_end_date).normalize()\n",
    "    df = df.loc[(df.index >= ms) & (df.index <= me)]\n",
    "    if df.empty:\n",
    "        return\n",
    "    \n",
    "    # 2) pick coin-level columns and melt\n",
    "    SUFFIXES = [\n",
    "        \"actual_position_size\",\n",
    "        \"actual_position_notional\",\n",
    "        \"target_size\",\n",
    "        \"target_notional\",\n",
    "        \"final_weighted_additive_signal\",\n",
    "        \"final_signal\",\n",
    "        \"t_1_close\",\n",
    "        \"open\",\n",
    "        \"20_avg_true_range_price\",\n",
    "        \"highest_high_56\",\n",
    "        \"stop_loss\",\n",
    "        \"stopout_flag\",\n",
    "        \"annualized_volatility_30\",\n",
    "        \"event\",\n",
    "    ]\n",
    "    # CONTEXT_COLS = [\"available_cash\", \"total_portfolio_value\"]  # optional\n",
    "    \n",
    "    ticker_cols = [f'{ticker}_{s}' for ticker in ticker_list for s in SUFFIXES]\n",
    "    wide = df[ticker_cols].copy()\n",
    "    long = (\n",
    "        wide\n",
    "        .stack()                       # index: (date, column_name)\n",
    "        .rename(\"value\")\n",
    "        .reset_index()\n",
    "        .rename(columns={\"level_0\":\"date\",\"level_1\":\"field\"})\n",
    "    )\n",
    "    \n",
    "    # 3) parse product_id and metric from \"BTC-USD_actual_position_size\"\n",
    "    sp = long[\"field\"].str.split(\"_\", n=1)\n",
    "    long[\"product_id\"] = sp.str[0]\n",
    "    long[\"metric\"]     = sp.str[1]\n",
    "    # non-coin fields (available_cash/total_portfolio_value) will have metric=None; drop or keep separately\n",
    "    coin_mask = long[\"product_id\"].str.contains(\"-\", na=False)\n",
    "    long = long[coin_mask]\n",
    "    \n",
    "    # 4) pivot metrics back to columns per (date, product_id)\n",
    "    pos = (\n",
    "        long.pivot_table(index=[\"date\",\"product_id\"], columns=\"metric\", values=\"value\", aggfunc=\"last\")\n",
    "            .reset_index()\n",
    "    )\n",
    "    \n",
    "    # 5) add partitions and enforce dtypes\n",
    "    pos[\"date\"]   = pd.to_datetime(pos[\"date\"]).dt.normalize()\n",
    "    pos[\"year\"]   = pos[\"date\"].dt.year.astype(\"int32\")\n",
    "    pos[\"month\"]  = pos[\"date\"].dt.month.astype(\"int32\")\n",
    "    pos[\"product_id\"] = pos[\"product_id\"].astype(\"string\")\n",
    "    \n",
    "    float_cols = [\"actual_position_size\",\"actual_position_notional\",\"target_size\",\n",
    "                  \"target_notional\",\"final_weighted_additive_signal\",\"final_signal\",\n",
    "                  \"t_1_close\",\"open\",\"20_avg_true_range_price\",\"highest_high_56\",\n",
    "                  \"stop_loss\",\"annualized_volatility_30\"]\n",
    "    for c in float_cols:\n",
    "        if c in pos.columns:\n",
    "            pos[c] = pd.to_numeric(pos[c], errors=\"coerce\").astype(\"float64\")\n",
    "    if \"stopout_flag\" in pos.columns:\n",
    "        pos[\"stopout_flag\"] = pos[\"stopout_flag\"].astype(\"boolean\")\n",
    "    if \"event\" in pos.columns:\n",
    "        pos[\"event\"] = pos[\"event\"].astype(\"string\")\n",
    "\n",
    "    # --- NEW: replace the partition to keep it idempotent ---\n",
    "    y, m = int(pos[\"year\"].iloc[0]), int(pos[\"month\"].iloc[0])\n",
    "    part_dir = POSITIONS_DIR / f\"year={y}\" / f\"month={m}\"\n",
    "    if part_dir.exists():\n",
    "        shutil.rmtree(part_dir)\n",
    "\n",
    "    # 6) write\n",
    "    pos.to_parquet(\n",
    "        POSITIONS_DIR,\n",
    "        engine=\"pyarrow\",\n",
    "        partition_cols=[\"year\",\"month\"],\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_prev_month_snapshot_wide(prev_month_end: pd.Timestamp) -> pd.DataFrame | None:\n",
    "    d = pd.Timestamp(prev_month_end).normalize()\n",
    "\n",
    "    # enforce a stable schema across all files\n",
    "    schema = pa.schema([\n",
    "        (\"date\", pa.timestamp(\"ns\")),       # use \"ms\" if you wrote ms\n",
    "        (\"ticker\", pa.string()),            # string with NA allowed\n",
    "        (\"actual_position_size\", pa.float64()),\n",
    "        (\"actual_position_notional\", pa.float64()),\n",
    "        (\"available_cash\", pa.float64()),\n",
    "        (\"total_portfolio_value\", pa.float64()),\n",
    "        (\"year\", pa.int32()),\n",
    "        (\"month\", pa.int32()),\n",
    "    ])\n",
    "\n",
    "    dset = ds.dataset(str(MONTH_END_OPEN_POSITION_DIR), format=\"parquet\",\n",
    "                      partitioning=\"hive\", schema=schema)\n",
    "\n",
    "    # read only that month partition\n",
    "    tbl = dset.to_table(\n",
    "        filter=(ds.field(\"year\") == d.year) & (ds.field(\"month\") == d.month)\n",
    "    )\n",
    "    snap = tbl.to_pandas()\n",
    "    if snap.empty:\n",
    "        return None\n",
    "\n",
    "    snap[\"date\"] = pd.to_datetime(snap[\"date\"]).dt.normalize()\n",
    "    snap = snap.loc[snap[\"date\"] == d]\n",
    "    if snap.empty:\n",
    "        return None\n",
    "\n",
    "    # pivot back to your wide shape\n",
    "    available_cash = float(snap[\"available_cash\"].iloc[0])\n",
    "    total_pv = float(snap[\"total_portfolio_value\"].iloc[0])\n",
    "\n",
    "    pos = snap.dropna(subset=[\"ticker\"])\n",
    "    wide = pd.DataFrame(index=[d], columns=[\"available_cash\", \"total_portfolio_value\"])\n",
    "    wide.loc[d, \"available_cash\"] = available_cash\n",
    "    wide.loc[d, \"total_portfolio_value\"] = total_pv\n",
    "\n",
    "    for _, r in pos.iterrows():\n",
    "        t = r[\"ticker\"]\n",
    "        wide.loc[d, f\"{t}_actual_position_size\"] = float(r[\"actual_position_size\"])\n",
    "        wide.loc[d, f\"{t}_actual_position_notional\"] = float(r[\"actual_position_notional\"])\n",
    "\n",
    "    return wide\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN MONTHLY LOOP (streamlined)\n",
    "# -----------------------------\n",
    "adv_quantile = 0.60\n",
    "high_low_quantile = 0.60\n",
    "\n",
    "## Month-End Date Range for Backtest\n",
    "# month_end_dates = pd.date_range(\n",
    "#     start=pd.Timestamp(\"2022-03-31\").date(),\n",
    "#     end=pd.Timestamp(\"2024-12-31\").date(),\n",
    "#     freq=\"M\"\n",
    "# )\n",
    "\n",
    "month_end_dates = pd.date_range(\n",
    "    start=pd.Timestamp(\"2024-04-30\").date(),\n",
    "    end=pd.Timestamp(\"2024-12-31\").date(),\n",
    "    freq=\"M\"\n",
    ")\n",
    "\n",
    "## Performance columns to persist for portfolio analysis\n",
    "PERF_COLS = [\n",
    "    \"daily_portfolio_volatility\", \"available_cash\", \"count_of_positions\",\n",
    "    \"total_actual_position_notional\", \"total_target_notional\",\n",
    "    \"total_portfolio_value\", \"total_portfolio_value_upper_limit\",\n",
    "]\n",
    "\n",
    "for prev_month_end in month_end_dates:\n",
    "    ## Pull Base Universe from Monthly Eligible Ticker List\n",
    "    print(\"Get Final Ticker List Based on Signal Strength on Last Month End Date!!\")\n",
    "    eligible_path = ELIGIBLE_DIR / f\"{pd.Timestamp(prev_month_end).date()}_eligible.parquet\"\n",
    "    df_eligible = pd.read_parquet(eligible_path)\n",
    "    base_universe = df_eligible[\"product_id\"].unique().tolist()\n",
    "\n",
    "    ## Get Month Start and End Dates\n",
    "    month_start_date = pd.Timestamp(prev_month_end).normalize() + pd.Timedelta(days=1)\n",
    "    month_end_date = (pd.Timestamp(prev_month_end) + pd.offsets.MonthEnd(1)).normalize()\n",
    "    print(month_start_date.date(), month_end_date.date(), base_universe)\n",
    "\n",
    "    ## Check if this is the first month in the run\n",
    "    is_first_month = (month_start_date.normalize() == pd.Timestamp(signal_start_date).normalize())\n",
    "    print(f\"Is the first month flag set: {is_first_month}!!\")\n",
    "\n",
    "    ## Pull Previous Month Available Cash, Total Portfolio Value and Open Positions if available\n",
    "    ## If not available, it is the first month and we start with the Initial Capital\n",
    "    print(\"Check if there are any open positions from prior month!!\")\n",
    "    previous_month_open_positions_df = load_prev_month_snapshot_wide(prev_month_end)\n",
    "    if previous_month_open_positions_df is not None:\n",
    "        prev_month_available_cash = previous_month_open_positions_df.loc[prev_month_end.normalize(), \"available_cash\"]\n",
    "        prev_month_total_portfolio_value = previous_month_open_positions_df.loc[prev_month_end.normalize(), \"total_portfolio_value\"]\n",
    "        prev_month_open_ticker_list = [\n",
    "            col.split(\"_\")[0]\n",
    "            for col in previous_month_open_positions_df.columns\n",
    "            if col.endswith(\"_actual_position_notional\")\n",
    "        ]\n",
    "        print(f\"Tickers with Open Positions at Prior Month-end: {prev_month_open_ticker_list}\")\n",
    "    else:\n",
    "        prev_month_open_ticker_list = []\n",
    "        prev_month_available_cash = cfg[\"run\"][\"initial_capital\"]\n",
    "        prev_month_total_portfolio_value = cfg[\"run\"][\"initial_capital\"]\n",
    "        print(\"No Open Positions at Prior Month-end!!\")\n",
    "\n",
    "    ## Create a frozen list of tickers with open positions that are not on the eligible ticker list\n",
    "    print(\"Create Final Ticker List and Frozen Ticker List!!\")\n",
    "    frozen_ticker_list = [t for t in prev_month_open_ticker_list if t not in base_universe]\n",
    "    final_month_ticker_list = base_universe + frozen_ticker_list\n",
    "    print(f\"Final Ticker List for Current Month: {final_month_ticker_list}\")\n",
    "\n",
    "    ## Get the liquidity metrics for all tickers by date\n",
    "    ## This is to exit positions if tickers fall off the daily eligible list based on liquidity criteria\n",
    "    print(\"Pull all eligible tickers based on Liquidity Metrics for every date in the current month!!\")\n",
    "    eligibility_by_date = build_eligibility_by_date_for_month(\n",
    "        client,\n",
    "        month_start_date=month_start_date.date(),\n",
    "        month_end_date=month_end_date.date(),\n",
    "        lookback_day_count=lookback_day_count,\n",
    "        adv_quantile=adv_quantile,\n",
    "        high_low_quantile=high_low_quantile,\n",
    "        product_list=base_universe,\n",
    "        max_workers=5,\n",
    "        batch_size=10,\n",
    "        between_batches_sleep=0.10,\n",
    "    )\n",
    "\n",
    "    ## Generate Trend Following Trading Signal\n",
    "    print(\"Generating Moving Average Ribbon Signal!!\")\n",
    "    df_trend = get_trend_donchian_signal_for_portfolio_with_rolling_r_sqr_vol_of_vol(\n",
    "        start_date=month_start_date - pd.Timedelta(days=warmup_days),\n",
    "        end_date=month_end_date,\n",
    "        ticker_list=final_month_ticker_list,\n",
    "        fast_mavg=fast_mavg,\n",
    "        slow_mavg=slow_mavg,\n",
    "        mavg_stepsize=mavg_stepsize,\n",
    "        mavg_z_score_window=mavg_z_score_window,\n",
    "        entry_rolling_donchian_window=entry_rolling_donchian_window,\n",
    "        exit_rolling_donchian_window=exit_rolling_donchian_window,\n",
    "        use_donchian_exit_gate=use_donchian_exit_gate,\n",
    "        ma_crossover_signal_weight=ma_crossover_signal_weight,\n",
    "        donchian_signal_weight=donchian_signal_weight,\n",
    "        weighted_signal_ewm_window=weighted_signal_ewm_window,\n",
    "        rolling_r2_window=rolling_r2_window,\n",
    "        lower_r_sqr_limit=lower_r_sqr_limit,\n",
    "        upper_r_sqr_limit=upper_r_sqr_limit,\n",
    "        r2_smooth_window=r2_smooth_window,\n",
    "        r2_confirm_days=r2_confirm_days,\n",
    "        log_std_window=log_std_window,\n",
    "        coef_of_variation_window=coef_of_variation_window,\n",
    "        vol_of_vol_z_score_window=vol_of_vol_z_score_window,\n",
    "        vol_of_vol_p_min=vol_of_vol_p_min,\n",
    "        r2_strong_threshold=r2_strong_threshold,\n",
    "        use_activation=use_activation,\n",
    "        tanh_activation_constant_dict=tanh_activation_constant_dict,\n",
    "        moving_avg_type=moving_avg_type,\n",
    "        long_only=long_only,\n",
    "        price_or_returns_calc=price_or_returns_calc,\n",
    "        use_coinbase_data=use_coinbase_data,\n",
    "        use_saved_files=False,\n",
    "        saved_file_end_date=saved_file_end_date,\n",
    "    )\n",
    "\n",
    "    ## Volatility Adjust the Trend Signal\n",
    "    print(\"Generating Volatility Adjusted Trend Signal!!\")\n",
    "    df_signal = size_cont.get_volatility_adjusted_trend_signal_continuous(\n",
    "        df=df_trend,\n",
    "        ticker_list=final_month_ticker_list,\n",
    "        volatility_window=volatility_window,\n",
    "        annual_trading_days=annual_trading_days,\n",
    "    )\n",
    "\n",
    "    ## Get Average True Range for all tickers\n",
    "    print(\"Getting Average True Range for Stop Loss Calculation!!\")\n",
    "    df_atr = size_cont.get_average_true_range_portfolio(\n",
    "        start_date=month_start_date - pd.Timedelta(days=warmup_days),\n",
    "        end_date=month_end_date,\n",
    "        ticker_list=final_month_ticker_list,\n",
    "        rolling_atr_window=rolling_atr_window,\n",
    "        highest_high_window=highest_high_window,\n",
    "        price_or_returns_calc=\"price\",\n",
    "        use_coinbase_data=use_coinbase_data,\n",
    "        use_saved_files=False,\n",
    "        saved_file_end_date=saved_file_end_date,\n",
    "    )\n",
    "    df_signal = pd.merge(df_signal, df_atr, left_index=True, right_index=True, how=\"left\")\n",
    "\n",
    "    ## Get the Volatility Targeted Position Sizes for the eligible tickers\n",
    "    print(\"Calculating Volatility Targeted Position Size and Cash Management!!\")\n",
    "    df_month = get_target_volatility_daily_portfolio_positions_expanded_universe(\n",
    "        df_signal,\n",
    "        ticker_list=final_month_ticker_list,\n",
    "        initial_capital=initial_capital,\n",
    "        rolling_cov_window=rolling_cov_window,\n",
    "        stop_loss_strategy=stop_loss_strategy,\n",
    "        rolling_atr_window=rolling_atr_window,\n",
    "        atr_multiplier=atr_multiplier,\n",
    "        highest_high_window=highest_high_window,\n",
    "        cash_buffer_percentage=cash_buffer_percentage,\n",
    "        annualized_target_volatility=annualized_target_volatility,\n",
    "        transaction_cost_est=transaction_cost_est,\n",
    "        passive_trade_rate=passive_trade_rate,\n",
    "        notional_threshold_pct=notional_threshold_pct,\n",
    "        min_trade_notional_abs=min_trade_notional_abs,\n",
    "        cooldown_counter_threshold=cooldown_counter_threshold,\n",
    "        annual_trading_days=annual_trading_days,\n",
    "        use_specific_start_date=is_first_month,\n",
    "        signal_start_date=signal_start_date,\n",
    "        start_date=month_start_date,\n",
    "        previous_month_open_positions_df=previous_month_open_positions_df,\n",
    "        frozen_ticker_list=frozen_ticker_list,\n",
    "        per_asset_weight_cap=0.25,\n",
    "        per_asset_notional_cap=None,\n",
    "        allow_adds_on_frozen=True,\n",
    "        eligibility_by_date=eligibility_by_date,\n",
    "        force_exit_if_ineligible=True,\n",
    "        max_positions=12,\n",
    "        max_new_per_day=3,\n",
    "        signal_col_suffix=\"_final_signal\",\n",
    "    )\n",
    "\n",
    "    ## Write the current month's portfolio performance metrics to a Parquet file\n",
    "    write_monthly_performance(df_month,\n",
    "                              cols=PERF_COLS,\n",
    "                              month_start=month_start_date,\n",
    "                              month_end=month_end_date\n",
    "                             )\n",
    "\n",
    "    ## Save the Open Positions at month end to a Parquet file\n",
    "    snapshot_open_positions_long(\n",
    "        df=df_month,\n",
    "        month_end_date=month_end_date,\n",
    "        ticker_list=final_month_ticker_list,\n",
    "    )\n",
    "\n",
    "    ## Archive the Daily Positions file by ticker for further analysis\n",
    "    write_daily_positions(df_month, ticker_list=final_month_ticker_list,\n",
    "                          month_start_date=month_start_date, month_end_date=month_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788508db-6404-437f-ba22-46f8c1673abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b5418-2a9e-44c2-8900-ca073fae7e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbdb0ef-c78e-494f-8ce9-a982322e50cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0beac-4fea-43f6-9c3e-dc7e0fdb8d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c2cd6-ab32-4ec1-8d74-f90c7e0c8327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271bdae3-4869-4ab0-b0d8-c3ac0359a936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32467f-9a87-46c0-a8b5-e6330f3be873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486aa894-90c6-4403-94a5-049e0c39a065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e4c2a2-8ed5-4c79-93b4-29b3cf4af3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6bb9e0-2d12-4120-91b6-87e5b599ccea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da00e16-b402-4c0f-a5b7-3c398b89233c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5440e37-321c-4556-8e10-15183be62722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a559fd1-ec07-48a5-9955-167c82ea5756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628aa3c-f917-4a37-ae29-fcf6073ad98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db49c15-e131-4b7f-ac1e-566ccd39fd13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475d23f-a2e5-42c4-a423-14d0c8d194c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
